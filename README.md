Of course bro! Here’s your full `README.md` in proper **Markdown format** — just copy and paste it directly into your project’s root folder.

---

````markdown
# 🧠 Document-Based Question Answering API (RAG Pipeline)

This project is a lightweight and production-style **question answering system** that lets users:

- Upload custom documents
- Ask questions
- Get context-aware answers generated by an LLM

Built using:
- 🔥 FastAPI
- 🤖 Hugging Face LLMs (Mistral-7B)
- 📦 FAISS for semantic vector search
- 💡 Retrieval-Augmented Generation (RAG) architecture

---

## 🚀 Features

- `POST /documents` → Ingest and index documents
- `GET /documents` → View uploaded documents
- `DELETE /documents/{id}` → Remove any document + its vectors
- `POST /query` → Ask questions; get smart answers from your own documents

---

## 🧠 How It Works (Architecture)

This API combines vector search + LLM to answer questions using your documents only.

```text
User Question → [Vector Embedding] → [FAISS Search] → [Top Chunks] → [LLM Prompt]
                                                                      ↓
                                             "Answer: <context-based answer>"
````

* 🔹 **RAG Pattern**: Retrieve relevant document chunks → Inject into LLM prompt → Generate grounded answer
* 🔹 **No hallucination**: Only uses uploaded content
* 🔹 **Fast & Free**: Uses Hugging Face’s Mistral model via Inference API

---

## 🧱 Tech Stack

| Component         | Tool / Library                                      |
| ----------------- | --------------------------------------------------- |
| Backend Framework | FastAPI                                             |
| LLM API           | Hugging Face (`mistralai/Mistral-7B-Instruct-v0.2`) |
| Embeddings        | `all-MiniLM-L6-v2` (SentenceTransformers)           |
| Vector Store      | FAISS (in-memory with metadata)                     |
| Env Mgmt          | `python-dotenv`                                     |

---

## 📂 Folder Structure

```
app/
├── api/               ← FastAPI routes
├── services/          ← LLM, Embedding, and FAISS logic
├── models/            ← Pydantic schemas
├── main.py            ← FastAPI app
├── vector_store/      ← Persisted FAISS + metadata
.env
README.md
```

---

## ⚙️ Setup Instructions

### ✅ Prerequisites

* Python 3.9+
* Hugging Face account

---

### 1️⃣ Clone the repo

```bash
git clone https://github.com/your-username/doc-qa-api.git
cd doc-qa-api
```

---

### 2️⃣ Install dependencies

```bash
pip install -r requirements.txt
```

---

### 3️⃣ Set your Hugging Face Token

Create a `.env` file:

```env
HF_TOKEN=hf_YourHuggingFaceTokenHere
```

---

### 4️⃣ Run the app

```bash
uvicorn app.main:app --reload
```

Visit: `http://localhost:8000/docs` for Swagger UI

---

## 🧪 Example Usage (via Swagger)

### 📄 Upload Document

```json
POST /documents
{
  "documents": [
    {
      "id": "ai_types",
      "title": "Types of AI",
      "content": "Artificial intelligence can be classified into narrow, general, and superintelligence..."
    }
  ]
}
```

---

### ❓ Ask a Question

```json
POST /query
{
  "question": "What are the types of AI?"
}
```

🔁 Returns:

```json
{
  "question": "What are the types of AI?",
  "answer": "The types of AI are narrow AI, general AI, and superintelligence...",
  "sources": [
    { "doc_id": "ai_types", "chunk_id": 0, "title": "Types of AI" }
  ]
}
```

---

## 🧼 Edge Case Handling

* ❌ Rejects empty documents or duplicate IDs
* ❌ Returns fallback if no relevant chunks found
* ✅ Limits long documents (chunk count)
* ✅ Keeps FAISS index and metadata in sync


## 🙋‍♂️ Author

👨‍💻 Built by **Rafay Khan**
💼 Backend Python Developer | AI Enthusiast
🔗 [LinkedIn](https://linkedin.com/in/your-profile)
🔗 [GitHub](https://github.com/your-username)

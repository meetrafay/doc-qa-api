Of course bro! Hereâ€™s your full `README.md` in proper **Markdown format** â€” just copy and paste it directly into your projectâ€™s root folder.

---

````markdown
# ğŸ§  Document-Based Question Answering API (RAG Pipeline)

This project is a lightweight and production-style **question answering system** that lets users:

- Upload custom documents
- Ask questions
- Get context-aware answers generated by an LLM

Built using:
- ğŸ”¥ FastAPI
- ğŸ¤– Hugging Face LLMs (Mistral-7B)
- ğŸ“¦ FAISS for semantic vector search
- ğŸ’¡ Retrieval-Augmented Generation (RAG) architecture

---

## ğŸš€ Features

- `POST /documents` â†’ Ingest and index documents
- `GET /documents` â†’ View uploaded documents
- `DELETE /documents/{id}` â†’ Remove any document + its vectors
- `POST /query` â†’ Ask questions; get smart answers from your own documents

---

## ğŸ§  How It Works (Architecture)

This API combines vector search + LLM to answer questions using your documents only.

```text
User Question â†’ [Vector Embedding] â†’ [FAISS Search] â†’ [Top Chunks] â†’ [LLM Prompt]
                                                                      â†“
                                             "Answer: <context-based answer>"
````

* ğŸ”¹ **RAG Pattern**: Retrieve relevant document chunks â†’ Inject into LLM prompt â†’ Generate grounded answer
* ğŸ”¹ **No hallucination**: Only uses uploaded content
* ğŸ”¹ **Fast & Free**: Uses Hugging Faceâ€™s Mistral model via Inference API

---

## ğŸ§± Tech Stack

| Component         | Tool / Library                                      |
| ----------------- | --------------------------------------------------- |
| Backend Framework | FastAPI                                             |
| LLM API           | Hugging Face (`mistralai/Mistral-7B-Instruct-v0.2`) |
| Embeddings        | `all-MiniLM-L6-v2` (SentenceTransformers)           |
| Vector Store      | FAISS (in-memory with metadata)                     |
| Env Mgmt          | `python-dotenv`                                     |

---

## ğŸ“‚ Folder Structure

```
app/
â”œâ”€â”€ api/               â† FastAPI routes
â”œâ”€â”€ services/          â† LLM, Embedding, and FAISS logic
â”œâ”€â”€ models/            â† Pydantic schemas
â”œâ”€â”€ main.py            â† FastAPI app
â”œâ”€â”€ vector_store/      â† Persisted FAISS + metadata
.env
README.md
```

---

## âš™ï¸ Setup Instructions

### âœ… Prerequisites

* Python 3.9+
* Hugging Face account

---

### 1ï¸âƒ£ Clone the repo

```bash
git clone https://github.com/your-username/doc-qa-api.git
cd doc-qa-api
```

---

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Set your Hugging Face Token

Create a `.env` file:

```env
HF_TOKEN=hf_YourHuggingFaceTokenHere
```

---

### 4ï¸âƒ£ Run the app

```bash
uvicorn app.main:app --reload
```

Visit: `http://localhost:8000/docs` for Swagger UI

---

## ğŸ§ª Example Usage (via Swagger)

### ğŸ“„ Upload Document

```json
POST /documents
{
  "documents": [
    {
      "id": "ai_types",
      "title": "Types of AI",
      "content": "Artificial intelligence can be classified into narrow, general, and superintelligence..."
    }
  ]
}
```

---

### â“ Ask a Question

```json
POST /query
{
  "question": "What are the types of AI?"
}
```

ğŸ” Returns:

```json
{
  "question": "What are the types of AI?",
  "answer": "The types of AI are narrow AI, general AI, and superintelligence...",
  "sources": [
    { "doc_id": "ai_types", "chunk_id": 0, "title": "Types of AI" }
  ]
}
```

---

## ğŸ§¼ Edge Case Handling

* âŒ Rejects empty documents or duplicate IDs
* âŒ Returns fallback if no relevant chunks found
* âœ… Limits long documents (chunk count)
* âœ… Keeps FAISS index and metadata in sync


## ğŸ™‹â€â™‚ï¸ Author

ğŸ‘¨â€ğŸ’» Built by **Rafay Khan**
ğŸ’¼ Backend Python Developer | AI Enthusiast
ğŸ”— [LinkedIn](https://linkedin.com/in/your-profile)
ğŸ”— [GitHub](https://github.com/your-username)
